#!/usr/bin/env python3
"""
æ–‡ä»¶å»é‡å™¨ - æ£€æµ‹å’Œåˆ é™¤é‡å¤æ–‡ä»¶å·¥å…·

åŠŸèƒ½:
- åŸºäºæ–‡ä»¶å†…å®¹æ£€æµ‹é‡å¤æ–‡ä»¶
- æ”¯æŒå¤šç§å“ˆå¸Œç®—æ³• (MD5, SHA1, SHA256)
- æ™ºèƒ½æ–‡ä»¶å¤§å°é¢„è¿‡æ»¤
- æ‰¹é‡åˆ é™¤é‡å¤æ–‡ä»¶
- ç”Ÿæˆé‡å¤æ–‡ä»¶æŠ¥å‘Š

ä½œè€…: ToolCollection
ç‰ˆæœ¬: 1.0.0
"""

import argparse
import hashlib
import json
import os
import sys
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple
import time


class FileDeduplicator:
    """æ–‡ä»¶å»é‡å™¨ç±»"""
    
    def __init__(self, hash_algorithm: str = 'md5', chunk_size: int = 8192):
        """
        åˆå§‹åŒ–æ–‡ä»¶å»é‡å™¨
        
        Args:
            hash_algorithm: å“ˆå¸Œç®—æ³• ('md5', 'sha1', 'sha256')
            chunk_size: è¯»å–æ–‡ä»¶å—å¤§å°
        """
        self.hash_algorithm = hash_algorithm.lower()
        self.chunk_size = chunk_size
        self.hash_func = getattr(hashlib, self.hash_algorithm)
        
    def calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            hash_obj = self.hash_func()
            with open(file_path, 'rb') as f:
                while chunk := f.read(self.chunk_size):
                    hash_obj.update(chunk)
            return hash_obj.hexdigest()
        except Exception as e:
            print(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return None
    
    def scan_directory(self, directory: str, extensions: List[str] = None, 
                      min_size: int = 0, max_size: int = None) -> Dict[str, List[str]]:
        """
        æ‰«æç›®å½•ï¼ŒæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
        
        Args:
            directory: æ‰«æç›®å½•
            extensions: æ–‡ä»¶æ‰©å±•åè¿‡æ»¤
            min_size: æœ€å°æ–‡ä»¶å¤§å° (å­—èŠ‚)
            max_size: æœ€å¤§æ–‡ä»¶å¤§å° (å­—èŠ‚)
            
        Returns:
            æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        size_groups = defaultdict(list)
        
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    file_path = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    if extensions:
                        file_ext = os.path.splitext(file)[1].lower()
                        if file_ext not in extensions:
                            continue
                    
                    # è·å–æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(file_path)
                    except OSError:
                        continue
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°èŒƒå›´
                    if file_size < min_size:
                        continue
                    if max_size and file_size > max_size:
                        continue
                    
                    size_groups[file_size].append(file_path)
            
            return size_groups
            
        except Exception as e:
            print(f"âŒ æ‰«æç›®å½•å¤±è´¥: {e}")
            return {}
    
    def find_duplicates(self, directory: str, extensions: List[str] = None,
                       min_size: int = 0, max_size: int = None) -> Dict[str, List[str]]:
        """
        æŸ¥æ‰¾é‡å¤æ–‡ä»¶
        
        Args:
            directory: æ‰«æç›®å½•
            extensions: æ–‡ä»¶æ‰©å±•åè¿‡æ»¤
            min_size: æœ€å°æ–‡ä»¶å¤§å° (å­—èŠ‚)
            max_size: æœ€å¤§æ–‡ä»¶å¤§å° (å­—èŠ‚)
            
        Returns:
            æŒ‰å“ˆå¸Œå€¼åˆ†ç»„çš„é‡å¤æ–‡ä»¶å­—å…¸
        """
        print(f"ğŸ” æ‰«æç›®å½•: {directory}")
        size_groups = self.scan_directory(directory, extensions, min_size, max_size)
        
        if not size_groups:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return {}
        
        print(f"ğŸ“Š æ‰¾åˆ° {sum(len(files) for files in size_groups.values())} ä¸ªæ–‡ä»¶")
        
        # æŒ‰å¤§å°åˆ†ç»„ï¼Œåªå¤„ç†æœ‰å¤šä¸ªæ–‡ä»¶çš„ç»„
        duplicate_candidates = {size: files for size, files in size_groups.items() 
                              if len(files) > 1}
        
        if not duplicate_candidates:
            print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
            return {}
        
        print(f"ğŸ” æ£€æŸ¥ {len(duplicate_candidates)} ä¸ªå¤§å°ç»„ä¸­çš„é‡å¤æ–‡ä»¶...")
        
        # è®¡ç®—å“ˆå¸Œå€¼å¹¶åˆ†ç»„
        hash_groups = defaultdict(list)
        total_files = sum(len(files) for files in duplicate_candidates.values())
        processed = 0
        
        for size, files in duplicate_candidates.items():
            for file_path in files:
                processed += 1
                if processed % 100 == 0:
                    print(f"  è¿›åº¦: {processed}/{total_files}")
                
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    hash_groups[file_hash].append(file_path)
        
        # åªè¿”å›æœ‰é‡å¤çš„ç»„
        duplicates = {hash_val: files for hash_val, files in hash_groups.items() 
                     if len(files) > 1}
        
        return duplicates
    
    def delete_duplicates(self, duplicates: Dict[str, List[str]], 
                         keep_strategy: str = 'oldest', dry_run: bool = True) -> Dict[str, List[str]]:
        """
        åˆ é™¤é‡å¤æ–‡ä»¶
        
        Args:
            duplicates: é‡å¤æ–‡ä»¶å­—å…¸
            keep_strategy: ä¿ç•™ç­–ç•¥ ('oldest', 'newest', 'smallest_path')
            dry_run: æ˜¯å¦åªé¢„è§ˆï¼Œä¸å®é™…åˆ é™¤
            
        Returns:
            å·²åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨
        """
        deleted_files = []
        
        for hash_val, files in duplicates.items():
            if len(files) <= 1:
                continue
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©è¦ä¿ç•™çš„æ–‡ä»¶
            if keep_strategy == 'oldest':
                files.sort(key=lambda x: os.path.getctime(x))
                keep_file = files[0]
            elif keep_strategy == 'newest':
                files.sort(key=lambda x: os.path.getctime(x))
                keep_file = files[-1]
            elif keep_strategy == 'smallest_path':
                keep_file = min(files, key=lambda x: len(x))
            else:
                keep_file = files[0]
            
            # åˆ é™¤å…¶ä»–æ–‡ä»¶
            for file_path in files:
                if file_path != keep_file:
                    if not dry_run:
                        try:
                            os.remove(file_path)
                            deleted_files.append(file_path)
                            print(f"ğŸ—‘ï¸  å·²åˆ é™¤: {file_path}")
                        except Exception as e:
                            print(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
                    else:
                        deleted_files.append(file_path)
                        print(f"ğŸ—‘ï¸  å°†åˆ é™¤: {file_path}")
            
            if not dry_run:
                print(f"âœ… ä¿ç•™: {keep_file}")
        
        return deleted_files
    
    def generate_report(self, duplicates: Dict[str, List[str]], 
                       output_file: str = None) -> Dict:
        """ç”Ÿæˆé‡å¤æ–‡ä»¶æŠ¥å‘Š"""
        report = {
            'scan_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'hash_algorithm': self.hash_algorithm,
            'total_duplicate_groups': len(duplicates),
            'total_duplicate_files': sum(len(files) for files in duplicates.values()),
            'potential_space_saved': 0,
            'duplicate_groups': []
        }
        
        for hash_val, files in duplicates.items():
            file_size = os.path.getsize(files[0])
            space_saved = file_size * (len(files) - 1)
            report['potential_space_saved'] += space_saved
            
            group_info = {
                'hash': hash_val,
                'file_count': len(files),
                'file_size': file_size,
                'space_saved': space_saved,
                'files': files
            }
            report['duplicate_groups'].append(group_info)
        
        # æŒ‰èŠ‚çœç©ºé—´æ’åº
        report['duplicate_groups'].sort(key=lambda x: x['space_saved'], reverse=True)
        
        if output_file:
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        
        return report
    
    def print_summary(self, duplicates: Dict[str, List[str]]):
        """æ‰“å°é‡å¤æ–‡ä»¶æ‘˜è¦"""
        if not duplicates:
            print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
            return
        
        total_groups = len(duplicates)
        total_files = sum(len(files) for files in duplicates.values())
        total_size = sum(os.path.getsize(files[0]) * (len(files) - 1) 
                        for files in duplicates.values())
        
        print(f"\nğŸ“Š é‡å¤æ–‡ä»¶æ‘˜è¦:")
        print(f"  é‡å¤ç»„æ•°: {total_groups}")
        print(f"  é‡å¤æ–‡ä»¶æ•°: {total_files}")
        print(f"  å¯èŠ‚çœç©ºé—´: {self.format_size(total_size)}")
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€å¤§çš„é‡å¤ç»„
        sorted_groups = sorted(duplicates.items(), 
                             key=lambda x: os.path.getsize(x[1][0]) * (len(x[1]) - 1),
                             reverse=True)
        
        print(f"\nğŸ” å‰5ä¸ªæœ€å¤§çš„é‡å¤ç»„:")
        for i, (hash_val, files) in enumerate(sorted_groups[:5], 1):
            file_size = os.path.getsize(files[0])
            space_saved = file_size * (len(files) - 1)
            print(f"  {i}. {len(files)} ä¸ªæ–‡ä»¶, æ¯ç»„ {self.format_size(file_size)}, "
                  f"å¯èŠ‚çœ {self.format_size(space_saved)}")
            for file_path in files[:3]:
                print(f"     - {file_path}")
            if len(files) > 3:
                print(f"     ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
    
    @staticmethod
    def format_size(size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        
        return f"{size_bytes:.1f} {size_names[i]}"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ–‡ä»¶å»é‡å™¨ - æ£€æµ‹å’Œåˆ é™¤é‡å¤æ–‡ä»¶å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ‰«æç›®å½•ï¼ŒæŸ¥æ‰¾é‡å¤æ–‡ä»¶
  python file_deduplicator.py /path/to/directory --scan
  
  # æŸ¥æ‰¾ç‰¹å®šç±»å‹çš„é‡å¤æ–‡ä»¶
  python file_deduplicator.py /path/to/directory --scan --extensions .jpg .png .mp4
  
  # åˆ é™¤é‡å¤æ–‡ä»¶ (ä¿ç•™æœ€æ—§çš„)
  python file_deduplicator.py /path/to/directory --delete --keep oldest
  
  # é¢„è§ˆåˆ é™¤æ“ä½œ
  python file_deduplicator.py /path/to/directory --delete --dry-run
  
  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  python file_deduplicator.py /path/to/directory --scan --report report.json
        """
    )
    
    parser.add_argument('directory', help='è¦æ‰«æçš„ç›®å½•è·¯å¾„')
    parser.add_argument('--scan', action='store_true', help='æ‰«æé‡å¤æ–‡ä»¶')
    parser.add_argument('--delete', action='store_true', help='åˆ é™¤é‡å¤æ–‡ä»¶')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…åˆ é™¤')
    parser.add_argument('--report', help='ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    
    # æ‰«æé€‰é¡¹
    parser.add_argument('--extensions', nargs='+', help='æ–‡ä»¶æ‰©å±•åè¿‡æ»¤ (å¦‚ .jpg .png)')
    parser.add_argument('--min-size', type=int, default=0, help='æœ€å°æ–‡ä»¶å¤§å° (å­—èŠ‚)')
    parser.add_argument('--max-size', type=int, help='æœ€å¤§æ–‡ä»¶å¤§å° (å­—èŠ‚)')
    parser.add_argument('--hash', choices=['md5', 'sha1', 'sha256'], 
                       default='md5', help='å“ˆå¸Œç®—æ³•')
    
    # åˆ é™¤é€‰é¡¹
    parser.add_argument('--keep', choices=['oldest', 'newest', 'smallest_path'], 
                       default='oldest', help='ä¿ç•™ç­–ç•¥')
    parser.add_argument('--chunk-size', type=int, default=8192, help='è¯»å–æ–‡ä»¶å—å¤§å°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.directory}")
        sys.exit(1)
    
    # åˆ›å»ºå»é‡å™¨
    deduplicator = FileDeduplicator(args.hash, args.chunk_size)
    
    # å¤„ç†æ–‡ä»¶æ‰©å±•å
    extensions = None
    if args.extensions:
        extensions = [ext.lower() if ext.startswith('.') else f'.{ext.lower()}' 
                     for ext in args.extensions]
    
    try:
        if args.scan or args.delete:
            # æŸ¥æ‰¾é‡å¤æ–‡ä»¶
            duplicates = deduplicator.find_duplicates(
                args.directory, extensions, args.min_size, args.max_size
            )
            
            if duplicates:
                # æ‰“å°æ‘˜è¦
                deduplicator.print_summary(duplicates)
                
                # ç”ŸæˆæŠ¥å‘Š
                if args.report:
                    deduplicator.generate_report(duplicates, args.report)
                
                # åˆ é™¤é‡å¤æ–‡ä»¶
                if args.delete:
                    print(f"\nğŸ—‘ï¸  å¼€å§‹åˆ é™¤é‡å¤æ–‡ä»¶ (ä¿ç•™ç­–ç•¥: {args.keep})")
                    if args.dry_run:
                        print("ğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
                    
                    deleted_files = deduplicator.delete_duplicates(
                        duplicates, args.keep, args.dry_run
                    )
                    
                    if args.dry_run:
                        print(f"\nğŸ“Š é¢„è§ˆç»“æœ: å°†åˆ é™¤ {len(deleted_files)} ä¸ªé‡å¤æ–‡ä»¶")
                    else:
                        print(f"\nâœ… åˆ é™¤å®Œæˆ: å·²åˆ é™¤ {len(deleted_files)} ä¸ªé‡å¤æ–‡ä»¶")
            else:
                print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
        
        else:
            print("ğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹ä½¿ç”¨è¯´æ˜")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 